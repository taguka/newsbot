{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "teVN4SM5Shzl",
    "outputId": "a6cc5dde-1e8b-4c0c-a229-ee588dfc2e2b"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext pyonmttok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VZEkT7r6YsOD",
    "outputId": "dfcbee69-99d1-478f-e6e4-3f1624fd54c1"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade keras tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "mwDpQGn4SLL1",
    "outputId": "57908295-eb3e-4156-b301-7798ec1ebabf"
   },
   "outputs": [],
   "source": [
    "!rm -f en_tg_train.tar.gz\n",
    "!wget https://www.dropbox.com/s/umd8tyx4wz1wquq/en_tg_train.tar.gz\n",
    "!rm -f en_tg_train.json\n",
    "!tar -xzvf en_tg_train.tar.gz\n",
    "!rm en_tg_train.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "gcsXXzshSYI3",
    "outputId": "5b35be9c-b418-4439-ec5b-83b958511465"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/no7x1n8acl5ykif/en_vectors_v2.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "iOVRqoR-yIXd",
    "outputId": "43a5c665-b831-48cd-9e80-fb9f5c21224d"
   },
   "outputs": [],
   "source": [
    "!rm -f all-the-news.zip\n",
    "!wget https://www.dropbox.com/s/bacg3cxckeqw6a9/all-the-news.zip\n",
    "!unzip all-the-news.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jS5W4zgaSc2G",
    "outputId": "9a11fc6e-1af9-422c-aeee-4b933f61e987"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model('en_vectors_v2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jhY9_QHcS_1U"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"en_tg_train.json\", \"r\") as r:\n",
    "    tg_data = json.load(r)\n",
    "tg_data.sort(key=lambda x: x['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "AORjTgMPWCpX",
    "outputId": "165184a9-09f4-45a4-84b7-16259333f55e"
   },
   "outputs": [],
   "source": [
    "!head articles1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "NY8YsPZdyPNx",
    "outputId": "f972a138-2e08-450e-adc6-6359c3737bf0"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import re\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def get_date(st):\n",
    "    dates = re.findall(r\"\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d\", st)\n",
    "    return next(iter(dates), None)\n",
    "\n",
    "all_the_news_files = (\"articles1.csv\", \"articles2.csv\", \"articles3.csv\")\n",
    "atn_data = []\n",
    "for file_name in all_the_news_files:\n",
    "    with open(file_name, \"r\") as r:\n",
    "        next(r)\n",
    "        reader = csv.reader(r, delimiter=',')\n",
    "        for row in reader:\n",
    "            _, _, _, host, _, date, _, _, _, text = row\n",
    "            if date == 'nan' or get_date(date) is None:\n",
    "                continue\n",
    "            atn_data.append({\"text\": text, \"site_name\": host, \"date\": date})\n",
    "atn_data.sort(key=lambda x: x[\"date\"])\n",
    "print(atn_data[0])\n",
    "print(len(atn_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6KoPumITgbB"
   },
   "outputs": [],
   "source": [
    "def words_to_embed(model, words):\n",
    "    vectors = [model.get_word_vector(w) for w in words]\n",
    "    norm_vectors = [x / np.linalg.norm(x) for x in vectors]\n",
    "    avg_wv = np.mean(norm_vectors, axis=0)\n",
    "    max_wv = np.max(norm_vectors, axis=0)\n",
    "    min_wv = np.min(norm_vectors, axis=0)\n",
    "    return np.concatenate((avg_wv, max_wv, min_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-PVkyLzUTm7m"
   },
   "outputs": [],
   "source": [
    "import pyonmttok\n",
    "tokenizer = pyonmttok.Tokenizer(\"conservative\", joiner_annotate=False)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).strip().replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").lower()\n",
    "    tokens, _ = tokenizer.tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "pTP-GBtXSx6R",
    "outputId": "c34f0aae-e7ab-42dc-f8ab-8ea9d57fbfab"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_samples(data, count):\n",
    "    last_host_end = {}\n",
    "    samples = []\n",
    "    for i, row in enumerate(data[:count]):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        host = row[\"site_name\"]\n",
    "        text = preprocess(row[\"text\"])\n",
    "        words = text.split(\" \")\n",
    "        if len(words) < 4:\n",
    "            continue\n",
    "        words = words[:500]\n",
    "            \n",
    "        border = len(words) // 2\n",
    "        begin_words = words[:border]\n",
    "        end_words = words[border:]\n",
    "\n",
    "        left_vector = words_to_embed(model, begin_words)\n",
    "        left_text = \" \".join(begin_words)\n",
    "        right_vector = words_to_embed(model, end_words)\n",
    "        right_text = \" \".join(end_words)\n",
    "\n",
    "        samples.append((left_vector, right_vector, left_text, right_text, 1))\n",
    "        if host in last_host_end:\n",
    "            samples.append((left_vector, last_host_end[host][0], left_text, last_host_end[host][1], 0))\n",
    "        last_host_end[host] = (right_vector, right_text)\n",
    "    return samples\n",
    "\n",
    "tg_samples = get_samples(tg_data, 100000)\n",
    "atn_samples = get_samples(atn_data, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HqblHw6DUZUG"
   },
   "outputs": [],
   "source": [
    "tg_test_size = len(tg_samples) // 10\n",
    "atn_test_size = len(atn_samples) // 10\n",
    "tg_test_samples = tg_samples[-tg_test_size:]\n",
    "train_samples = tg_samples[:-tg_test_size] + atn_samples[:-atn_test_size]\n",
    "test_samples = tg_test_samples + atn_samples[-atn_test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9J3kXsXfUy7m",
    "outputId": "2c41ae93-67c5-4f42-9b97-061bd805dbc9"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy import spatial\n",
    "\n",
    "scores = []\n",
    "test_y = []\n",
    "for sample in test_samples:\n",
    "    left_vector, right_vector, _, _, y = sample\n",
    "    test_y.append(y)\n",
    "    scores.append(-spatial.distance.cosine(left_vector, right_vector))\n",
    "metrics.roc_auc_score(test_y, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "lrWCJwlcVBhH",
    "outputId": "1666738e-2998-46e5-f4e6-fd22ce391c18"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dot\n",
    "from keras.models import Model\n",
    "\n",
    "left_input = Input(shape=(150,), dtype='float32')\n",
    "right_input = Input(shape=(150,), dtype='float32')\n",
    "dense = Dense(50, activation='linear')\n",
    "left_dense = dense(left_input)\n",
    "right_dense = dense(right_input)\n",
    "dot_layer = Dense(1, activation='sigmoid')(Dot(axes=1, normalize=True)([left_dense, right_dense]))\n",
    "nn_model = Model(inputs=[left_input, right_input], output=dot_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LJ-X1NnVUcw"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "nn_model.compile(optimizer=optimizers.Adam(lr=0.4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Nq5WBl-yVXAb",
    "outputId": "d01229c7-4c19-4e1c-ea82-4e1c2de46026"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "train_left = []\n",
    "train_right = []\n",
    "train_y = []\n",
    "random.shuffle(train_samples)\n",
    "for sample in train_samples:\n",
    "    left_vector, right_vector, left_text, right_text, y = sample\n",
    "    train_left.append(left_vector)\n",
    "    train_right.append(right_vector)\n",
    "    train_y.append(y)\n",
    "\n",
    "random.shuffle(test_samples)\n",
    "test_left = []\n",
    "test_right = []\n",
    "test_y = []\n",
    "for sample in test_samples:\n",
    "    left_vector, right_vector, _, _, y = sample\n",
    "    test_left.append(left_vector)\n",
    "    test_right.append(right_vector)\n",
    "    test_y.append(y)\n",
    "\n",
    "nn_model.fit([np.array(train_left), np.array(train_right)],\n",
    "             np.array(train_y),\n",
    "             batch_size=256,\n",
    "             epochs=100,\n",
    "             callbacks=[es,],\n",
    "             validation_data=([np.array(test_left), np.array(test_right)], np.array(test_y)),\n",
    "             verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "avMGX3wduUZx",
    "outputId": "9ebfef98-bedc-4cfd-aab5-60fa0d0de550"
   },
   "outputs": [],
   "source": [
    "embedder = Model(inputs=[left_input, ], output=left_dense)\n",
    "tg_test_left = []\n",
    "tg_test_right = []\n",
    "test_y = []\n",
    "for sample in tg_test_samples:\n",
    "    tg_left, tg_right, _, _, y = sample\n",
    "    tg_test_left.append(tg_left)\n",
    "    tg_test_right.append(tg_right)\n",
    "    test_y.append(y)\n",
    "pred_left = embedder.predict([np.array(tg_test_left)])\n",
    "pred_right = embedder.predict([np.array(tg_test_right)])\n",
    "scores = []\n",
    "for left, right in zip(pred_left, pred_right):\n",
    "    left = left / np.linalg.norm(left)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    score = (left.dot(right) + 1.0) / 2.0 - 1.0\n",
    "    scores.append(score)\n",
    "metrics.roc_auc_score(test_y, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkHxYX-EewGq"
   },
   "outputs": [],
   "source": [
    "matrix = dense.get_weights()[0]\n",
    "bias = dense.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClPcY2QYexbS"
   },
   "outputs": [],
   "source": [
    "with open(\"matrix.txt\", \"w\") as w:\n",
    "    for row_num in range(matrix.shape[1]):\n",
    "        row = []\n",
    "        for col_num in range(matrix.shape[0]):\n",
    "            row.append(float(matrix[col_num][row_num]))\n",
    "        w.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "with open(\"bias.txt\", \"w\") as w:\n",
    "    for value in bias:\n",
    "        w.write(\"{}\\n\".format(value))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SimilarityEn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
