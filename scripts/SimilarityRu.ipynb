{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "colab_type": "code",
    "id": "teVN4SM5Shzl",
    "outputId": "58dea9a1-3e06-4b50-dfb9-1898ba195e4f"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext pyonmttok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VZEkT7r6YsOD",
    "outputId": "de71158e-e4c7-46f0-e204-744a570b5f61"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade keras tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "colab_type": "code",
    "id": "mwDpQGn4SLL1",
    "outputId": "cac67674-76ad-49c1-d456-d392a11d8010"
   },
   "outputs": [],
   "source": [
    "!rm -f ru_tg_train.tar.gz\n",
    "!wget https://www.dropbox.com/s/1ecl9orr2tagcgi/ru_tg_train.tar.gz\n",
    "!rm -f ru_tg_train.json\n",
    "!tar -xzvf ru_tg_train.tar.gz\n",
    "!rm ru_tg_train.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "colab_type": "code",
    "id": "iOVRqoR-yIXd",
    "outputId": "0a75616c-ebe7-496e-96f1-d38852f857e8"
   },
   "outputs": [],
   "source": [
    "!rm -f lenta-ru-news.csv.gz\n",
    "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
    "!rm -f lenta-ru-news.csv\n",
    "!gzip -d lenta-ru-news.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "gcsXXzshSYI3",
    "outputId": "a82f59a0-b194-47a5-a2c1-ede5ebaa33a7"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/2nx97d8nzbzusee/ru_vectors_v2.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jS5W4zgaSc2G",
    "outputId": "abf65981-7254-4be2-9336-68206c69d46e"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model('ru_vectors_v2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jhY9_QHcS_1U"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ru_tg_train.json\", \"r\") as r:\n",
    "    tg_data = json.load(r)\n",
    "tg_data.sort(key=lambda x: x['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NY8YsPZdyPNx"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def get_date(url):\n",
    "    dates = re.findall(r\"\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d\", url)\n",
    "    return next(iter(dates), None)\n",
    "\n",
    "with open(\"lenta-ru-news.csv\", \"r\") as r:\n",
    "    next(r)\n",
    "    reader = csv.reader(r, delimiter=',')\n",
    "    lenta_data = []\n",
    "    for row in reader:\n",
    "        url, _, text, _, _ = row\n",
    "        date = get_date(url)\n",
    "        lenta_data.append({\"date\": date, \"text\": text, \"site_name\": \"lenta\"})\n",
    "\n",
    "lenta_data.sort(key=lambda x: x[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vvsECav_0I9D",
    "outputId": "c6997058-1532-4769-fae3-fa90f3363646"
   },
   "outputs": [],
   "source": [
    "len(lenta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6KoPumITgbB"
   },
   "outputs": [],
   "source": [
    "def words_to_embed(model, words):\n",
    "    vectors = [model.get_word_vector(w) for w in words]\n",
    "    norm_vectors = [x / np.linalg.norm(x) for x in vectors]\n",
    "    avg_wv = np.mean(norm_vectors, axis=0)\n",
    "    max_wv = np.max(norm_vectors, axis=0)\n",
    "    min_wv = np.min(norm_vectors, axis=0)\n",
    "    return np.concatenate((avg_wv, max_wv, min_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-PVkyLzUTm7m"
   },
   "outputs": [],
   "source": [
    "import pyonmttok\n",
    "tokenizer = pyonmttok.Tokenizer(\"conservative\", joiner_annotate=False)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).strip().replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").lower()\n",
    "    tokens, _ = tokenizer.tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pTP-GBtXSx6R",
    "outputId": "83631efe-aaca-4b78-fc3d-511fd06e877e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_samples(data, count):\n",
    "    last_host_end = {}\n",
    "    samples = []\n",
    "    for count, row in enumerate(data[:count]):\n",
    "        if count % 10000 == 0:\n",
    "            print(count)\n",
    "        \n",
    "        host = row['site_name']\n",
    "        text = preprocess(row['text'])\n",
    "        words = text.split(\" \")\n",
    "        if len(words) < 4:\n",
    "            continue\n",
    "        words = words[:300]\n",
    "            \n",
    "        border = len(words) // 2\n",
    "        begin_words = words[:border]\n",
    "        end_words = words[border:]\n",
    "\n",
    "        left_vector = words_to_embed(model, begin_words)\n",
    "        left_text = \" \".join(begin_words)\n",
    "        right_vector = words_to_embed(model, end_words)\n",
    "        right_text = \" \".join(end_words)\n",
    "\n",
    "        samples.append((left_vector, right_vector, left_text, right_text, 1))\n",
    "        if host in last_host_end:\n",
    "            samples.append((left_vector, last_host_end[host][0], left_text, last_host_end[host][1], 0))\n",
    "        last_host_end[host] = (right_vector, right_text)\n",
    "    return samples\n",
    "\n",
    "tg_samples = get_samples(tg_data, 100000)\n",
    "lenta_samples = get_samples(lenta_data, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HqblHw6DUZUG"
   },
   "outputs": [],
   "source": [
    "tg_test_size = len(tg_samples) // 10\n",
    "lenta_test_size = len(lenta_samples) // 10\n",
    "tg_test_samples = tg_samples[-tg_test_size:]\n",
    "train_samples = tg_samples[:-tg_test_size] + lenta_samples[:-lenta_test_size]\n",
    "test_samples = tg_test_samples + lenta_samples[-lenta_test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9J3kXsXfUy7m",
    "outputId": "af614f1a-e9ec-41af-d236-731202323684"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy import spatial\n",
    "\n",
    "scores = []\n",
    "test_y = []\n",
    "for sample in test_samples:\n",
    "    left_vector, right_vector, _, _, y = sample\n",
    "    test_y.append(y)\n",
    "    scores.append(-spatial.distance.cosine(left_vector, right_vector))\n",
    "metrics.roc_auc_score(test_y, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "lrWCJwlcVBhH",
    "outputId": "7fc829a1-8242-4aae-b3d4-2147f961e5ae"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dot\n",
    "from keras.models import Model\n",
    "\n",
    "left_input = Input(shape=(150,), dtype='float32')\n",
    "right_input = Input(shape=(150,), dtype='float32')\n",
    "dense = Dense(50, activation='linear')\n",
    "left_dense = dense(left_input)\n",
    "right_dense = dense(right_input)\n",
    "dot_layer = Dense(1, activation='sigmoid')(Dot(axes=1, normalize=True)([left_dense, right_dense]))\n",
    "nn_model = Model(inputs=[left_input, right_input], output=dot_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LJ-X1NnVUcw"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "nn_model.compile(optimizer=optimizers.Adam(lr=0.3), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "Nq5WBl-yVXAb",
    "outputId": "35060499-0ac0-401f-abb1-3e7cd44b6bdc"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "train_left = []\n",
    "train_right = []\n",
    "train_y = []\n",
    "for sample in train_samples:\n",
    "    left_vector, right_vector, left_text, right_text, y = sample\n",
    "    train_left.append(left_vector)\n",
    "    train_right.append(right_vector)\n",
    "    train_y.append(y)\n",
    "\n",
    "test_left = []\n",
    "test_right = []\n",
    "test_y = []\n",
    "for sample in test_samples:\n",
    "    left_vector, right_vector, _, _, y = sample\n",
    "    test_left.append(left_vector)\n",
    "    test_right.append(right_vector)\n",
    "    test_y.append(y)\n",
    "\n",
    "nn_model.fit([np.array(train_left), np.array(train_right)],\n",
    "             np.array(train_y),\n",
    "             batch_size=64,\n",
    "             epochs=100,\n",
    "             callbacks=[es,],\n",
    "             validation_data=([np.array(test_left), np.array(test_right)], np.array(test_y)),\n",
    "             verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "avMGX3wduUZx",
    "outputId": "56b41d4a-8ef2-4162-e1ff-b0b08ccd4a6e"
   },
   "outputs": [],
   "source": [
    "embedder = Model(inputs=[left_input, ], output=left_dense)\n",
    "tg_test_left = []\n",
    "tg_test_right = []\n",
    "test_y = []\n",
    "for sample in tg_test_samples:\n",
    "    tg_left, tg_right, _, _, y = sample\n",
    "    tg_test_left.append(tg_left)\n",
    "    tg_test_right.append(tg_right)\n",
    "    test_y.append(y)\n",
    "pred_left = embedder.predict([np.array(tg_test_left)])\n",
    "pred_right = embedder.predict([np.array(tg_test_right)])\n",
    "scores = []\n",
    "for left, right in zip(pred_left, pred_right):\n",
    "    left = left / np.linalg.norm(left)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    score = (left.dot(right) + 1.0) / 2.0 - 1.0\n",
    "    scores.append(score)\n",
    "metrics.roc_auc_score(test_y, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkHxYX-EewGq"
   },
   "outputs": [],
   "source": [
    "matrix = dense.get_weights()[0]\n",
    "bias = dense.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClPcY2QYexbS"
   },
   "outputs": [],
   "source": [
    "with open(\"matrix.txt\", \"w\") as w:\n",
    "    for row_num in range(matrix.shape[1]):\n",
    "        row = []\n",
    "        for col_num in range(matrix.shape[0]):\n",
    "            row.append(float(matrix[col_num][row_num]))\n",
    "        w.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "with open(\"bias.txt\", \"w\") as w:\n",
    "    for value in bias:\n",
    "        w.write(\"{}\\n\".format(value))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SimilarityRu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
