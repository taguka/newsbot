{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SimilarityRuTorchTripletLoss.ipynb","provenance":[],"authorship_tag":"ABX9TyMn7WBI4H5AwADYIJVBG9la"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"CumbEP2iZBG6"},"source":["!pip install fasttext pyonmttok\n","!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuq6nzRTZEpo"},"source":["!wget -nc https://www.dropbox.com/s/dpz0k0ypbql4rbt/ru_tg_1101_0510.jsonl.tar.gz -O - | tar -xz > ru_tg_1101_0510.jsonl\n","!wget https://www.dropbox.com/s/rohop0gt3zr2msm/ru_vectors_v1.bin"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_7MeyX5TZla0"},"source":["!rm -f lenta-ru-news.csv.gz\n","!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n","!rm -f lenta-ru-news.csv\n","!gzip -d lenta-ru-news.csv.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyjJhySGZoa8","executionInfo":{"status":"ok","timestamp":1627296248739,"user_tz":-120,"elapsed":14092,"user":{"displayName":"Tetyana Gavrylenko","photoUrl":"","userId":"16876190095093699565"}}},"source":["import json\n","\n","tg_data = []\n","with open(\"ru_tg_1101_0510.jsonl\", \"r\") as r:\n","    for line in r:\n","        tg_data.append(json.loads(line))\n","tg_data.sort(key=lambda x: x['timestamp'])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"pjqDm6wjZqTq","executionInfo":{"status":"ok","timestamp":1627296299226,"user_tz":-120,"elapsed":28848,"user":{"displayName":"Tetyana Gavrylenko","photoUrl":"","userId":"16876190095093699565"}}},"source":["import csv\n","import re\n","\n","def get_date(url):\n","    dates = re.findall(r\"\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d\", url)\n","    return next(iter(dates), None)\n","\n","with open(\"lenta-ru-news.csv\", \"r\") as r:\n","    next(r)\n","    reader = csv.reader(r, delimiter=',')\n","    lenta_data = []\n","    for row in reader:\n","        url, title, text, _, _ = row\n","        date = get_date(url)\n","        lenta_data.append({\"date\": date, \"text\": text, \"site_name\": \"lenta\", \"title\": title})\n","\n","lenta_data.sort(key=lambda x: x[\"date\"])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-2vFN97ZsjH","executionInfo":{"status":"ok","timestamp":1627296300048,"user_tz":-120,"elapsed":825,"user":{"displayName":"Tetyana Gavrylenko","photoUrl":"","userId":"16876190095093699565"}},"outputId":"8ad689df-9ff4-46e8-9521-68cc72b4b0c6"},"source":["import pyonmttok\n","import fasttext\n","\n","ft_model = fasttext.load_model('ru_vectors_v1.bin')\n","tokenizer = pyonmttok.Tokenizer(\"conservative\", joiner_annotate=False)\n","\n","def words_to_embed(model, words):\n","    vectors = [model.get_word_vector(w) for w in words]\n","    norm_vectors = [x / np.linalg.norm(x) for x in vectors]\n","    avg_wv = np.mean(norm_vectors, axis=0)\n","    max_wv = np.max(norm_vectors, axis=0)\n","    min_wv = np.min(norm_vectors, axis=0)\n","    return np.concatenate((avg_wv, max_wv, min_wv))\n","\n","def preprocess(tokenizer, text):\n","    text = str(text).strip().replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").lower()\n","    tokens, _ = tokenizer.tokenize(text)\n","    text = \" \".join(tokens)\n","    return text"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xeZTK2LoZwvs","executionInfo":{"status":"ok","timestamp":1627298618833,"user_tz":-120,"elapsed":2318787,"user":{"displayName":"Tetyana Gavrylenko","photoUrl":"","userId":"16876190095093699565"}},"outputId":"ee874056-c6d1-402b-a623-4e19546dadae"},"source":["import numpy as np\n","\n","def get_samples(data, ft_model, tokenizer, count, min_words=4, max_words=300):\n","    last_host_end = {}\n","    samples = []\n","    for count, row in enumerate(data[:count]):\n","        if count % 10000 == 0:\n","            print(count)\n","        \n","        host = row[\"site_name\"]\n","        text = preprocess(tokenizer, row[\"title\"] + \" \" + row[\"text\"])\n","        words = text.split(\" \")\n","        if len(words) < min_words:\n","            continue\n","        words = words[:max_words]\n","            \n","        border = len(words) // 2\n","        begin_words = words[:border]\n","        end_words = words[border:]\n","\n","        left_vector = words_to_embed(ft_model, begin_words)\n","        left_text = \" \".join(begin_words)\n","        right_vector = words_to_embed(ft_model, end_words)\n","        right_text = \" \".join(end_words)\n","\n","        if host in last_host_end:\n","            samples.append((left_vector, right_vector, last_host_end[host][0]))\n","        last_host_end[host] = (right_vector, right_text)\n","    return samples\n","\n","tg_samples = get_samples(tg_data, ft_model, tokenizer, 250000)\n","lenta_samples = get_samples(lenta_data, ft_model, tokenizer, 250000)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","130000\n","140000\n","150000\n","160000\n","170000\n","180000\n","190000\n","200000\n","210000\n","220000\n","230000\n","240000\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","130000\n","140000\n","150000\n","160000\n","170000\n","180000\n","190000\n","200000\n","210000\n","220000\n","230000\n","240000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pvyqSm1xZy8_","executionInfo":{"status":"ok","timestamp":1627298618850,"user_tz":-120,"elapsed":28,"user":{"displayName":"Tetyana Gavrylenko","photoUrl":"","userId":"16876190095093699565"}}},"source":["tg_test_size = len(tg_samples) // 10\n","lenta_test_size = len(lenta_samples) // 10\n","train_samples = tg_samples[:-tg_test_size] + lenta_samples[:-lenta_test_size]\n","test_samples = tg_samples[-tg_test_size:] + lenta_samples[-lenta_test_size:]\n","tg_test_samples = tg_samples[-tg_test_size:]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3j_oT6dZ0t_","executionInfo":{"status":"ok","timestamp":1627298625654,"user_tz":-120,"elapsed":6823,"user":{"displayName":"Tetyana Gavrylenko","photoUrl":"","userId":"16876190095093699565"}},"outputId":"31adb861-0c3a-4b71-c7e7-60e08ff3f7c4"},"source":["from sklearn import metrics\n","from scipy import spatial\n","\n","scores = []\n","test_y = []\n","for sample in test_samples:\n","    left_vector, pos_right_vector, neg_right_vector = sample\n","    test_y += [1, 0]\n","    scores.append(-spatial.distance.cosine(left_vector, pos_right_vector))\n","    scores.append(-spatial.distance.cosine(left_vector, neg_right_vector))\n","metrics.roc_auc_score(test_y, scores)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8358113239280407"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"s12rjnDRar3p"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"y5z9T2BFaxKX"},"source":["import torch\n","import torch.nn as nn\n","\n","class SiamiseModelTripletLoss(nn.Module):\n","    def __init__(self, embedding_dim=384, hidden_dim=50):\n","        super().__init__()\n","        \n","        self.mapping_layer = nn.Linear(embedding_dim, hidden_dim)\n","        # self.mapping_layer_1 = nn.Linear(embedding_dim, hidden_dim)\n","        # self.tanh = nn.Tanh()\n","        # self.mapping_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.distance = nn.PairwiseDistance(p=2)\n","        self.margin = 0.3\n","    \n","    def build_projections(self, in_vectors):\n","        projections = self.mapping_layer(in_vectors)\n","        norm = projections.norm(p=2, dim=1, keepdim=True)\n","        projections = projections.div(norm)\n","        return projections\n","\n","    def forward(self, pivot_vectors, positive_vectors, negative_vectors):\n","        pivot = self.build_projections(pivot_vectors)\n","        positive = self.build_projections(positive_vectors)\n","        negative = self.build_projections(negative_vectors)\n","        distances = self.distance(pivot, positive) - self.distance(pivot, negative) + self.margin\n","        loss = torch.mean(torch.max(distances, torch.zeros_like(distances)))\n","        return loss\n","    \n","    def apply(self, vectors):\n","        return self.build_projections(vectors)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFtKxndla2mp"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"h0SWXz3xa6cp"},"source":["import random\n","import time\n","import torch.optim as optim\n","\n","def get_next_gen_batch(samples, batch_size=64):\n","    indices = np.arange(len(samples))\n","    np.random.shuffle(indices)\n","    batch_begin = 0\n","    while batch_begin < len(samples):\n","        batch_indices = indices[batch_begin: batch_begin + batch_size]\n","        pivot_vectors = []\n","        positive_vectors = []\n","        negative_vectors = []\n","        for data_ind in batch_indices:\n","            pivot, positive, negative = samples[data_ind]\n","            pivot_vectors.append(pivot)\n","            positive_vectors.append(positive)\n","            negative_vectors.append(negative)\n","        batch_begin += batch_size\n","        yield torch.cuda.FloatTensor(pivot_vectors), torch.cuda.FloatTensor(positive_vectors), torch.cuda.FloatTensor(negative_vectors)\n","\n","def train_model(model, train_samples, val_samples, epochs_count=10, \n","                loss_every_nsteps=10000, lr=0.01, device_name=\"cuda\"):\n","    device = torch.device(device_name)\n","    model = model.to(device)\n","    total_loss = 0\n","    start_time = time.time()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_function = nn.BCELoss().cuda()\n","    prev_avg_val_loss = None\n","    for epoch in range(epochs_count):\n","        model.train()\n","        for step, (pivot, positive, negative) in enumerate(get_next_gen_batch(train_samples)):\n","            loss = model(pivot, positive, negative)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            total_loss += loss.item()\n","            if step % loss_every_nsteps == 0:\n","                val_total_loss = 0\n","                val_batch_count = 0\n","                model.eval()\n","                for _, (pivot, positive, negative) in enumerate(get_next_gen_batch(val_samples)):\n","                    val_total_loss += model(pivot, positive, negative)\n","                    val_batch_count += 1\n","                avg_val_loss = val_total_loss/val_batch_count\n","                print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n","                total_loss = 0\n","                start_time = time.time()\n","\n","random.shuffle(train_samples)\n","random.shuffle(test_samples)\n","model = SiamiseModelTripletLoss()\n","train_model(model, train_samples, test_samples)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYCuNN1ja-Wj"},"source":["# Testing"]},{"cell_type":"code","metadata":{"id":"S0RXXCTVbAh-"},"source":["tg_test_left = []\n","tg_test_right = []\n","test_y = []\n","for sample in tg_test_samples:\n","    tg_left, tg_pos_right, tg_neg_right = sample\n","    tg_test_left += [tg_left, tg_left]\n","    tg_test_right += [tg_pos_right, tg_neg_right]\n","    test_y += [1, 0]\n","\n","batch = []\n","batch_start = 0\n","nrows = len(tg_test_left)\n","scores = []\n","while batch_start < nrows:\n","    batch_end = batch_start + 32\n","    left_batch = tg_test_left[batch_start: batch_end]\n","    right_batch = tg_test_right[batch_start: batch_end]\n","    left = model.apply(torch.cuda.FloatTensor(left_batch)).cpu().detach().numpy()\n","    right = model.apply(torch.cuda.FloatTensor(right_batch)).t().cpu().detach().numpy()\n","    left = left / np.linalg.norm(left)\n","    right = right / np.linalg.norm(right)\n","    score = (left.dot(right) + 1.0) / 2.0 - 1.0\n","    score = np.diag(score)\n","    scores.extend(score.tolist())\n","    batch_start = batch_end\n","metrics.roc_auc_score(test_y, scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQ5pF2h4bFjH"},"source":["# Saving"]},{"cell_type":"code","metadata":{"id":"7H9XaAVRbHoH"},"source":["model = model.cpu()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LDfLriJ2bJBF"},"source":["import torch\n","import torch.nn as nn\n","\n","class Embedder(nn.Module):\n","    def __init__(self, embedding_dim=384, hidden_dim=50):\n","        super().__init__()\n","        \n","        self.mapping_layer = nn.Linear(embedding_dim, hidden_dim)\n","    \n","    def forward(self, in_vectors):\n","        projections = self.mapping_layer(in_vectors)\n","        norm = projections.norm(p=2, dim=1, keepdim=True)\n","        projections = projections.div(norm)\n","        return projections\n","\n","examples = torch.zeros((1, 384))\n","examples[0][:] = torch.FloatTensor(tg_test_samples[0][0])\n","embedder = Embedder()\n","embedder.mapping_layer.weight.data = model.mapping_layer.weight.data\n","embedder.mapping_layer.bias = model.mapping_layer.bias\n","traced_embedder = torch.jit.trace(embedder.cpu(), examples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0iaOQI6bLes"},"source":["torch.save(model.state_dict(), \"ru_full_model.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rh3g1C35bNL0"},"source":["traced_embedder.save(\"ru_sentence_embedder_v1.pt\")"],"execution_count":null,"outputs":[]}]}